batch_size: 16
iters: 20000

model:
  type: CX_Uper_4B
  backb: convnext_small
  in_channels: 3
  num_classes: 14
  dropout_rate: 0
  hsi_chs: 116

loss:
  types:
    - type: MixedLoss
      losses:
        - type: CrossEntropyLoss_Smooth
          smoothing: 0.05
        - type: DiceLoss
      coef: [1, 0.5]
  coef: [1]



optimizer:
  type: AdamW

lr_scheduler:
  type: StepDecay
  learning_rate: 0.0002
  step_size: 5000
  gamma: 0.5


train_dataset:
  type: RS_MD3B
  dataset_root: /home/aistudio/data/src/C2Seg_BW/train/
  train_path:  /home/aistudio/data/src/train.txt
  num_classes: 14
  transforms:
    - type: Normalize2
      mean1: [511.299, 664.097, 769.063, 1248.104, -15.968, -24.247]
      mean2: [999.433, 986.904, 974.447, 960.982, 947.575, 935.621, 924.243, 915.542, 907.726, 905.250, 909.091, 907.068, 902.661, 901.638, 902.661, 905.772, 911.386, 915.995, 920.723, 926.349, 931.746, 938.978, 946.329, 961.263, 959.789, 957.446, 954.460, 955.459, 954.627, 948.609, 945.025, 944.374, 949.510, 955.580, 960.178, 970.076, 975.216, 984.512, 990.831, 985.008, 950.914, 919.385, 889.296, 862.777, 825.307, 788.723, 762.220, 743.140, 734.277, 732.632, 733.673, 742.012, 753.558, 775.226, 794.269, 801.276, 799.636, 804.197, 816.373, 821.174, 854.051, 868.974, 869.269, 866.709, 876.802, 871.139, 835.212, 802.712, 767.638, 740.126, 713.945, 702.162, 696.747, 712.822, 763.065, 827.185, 896.042, 921.248, 940.171, 972.243, 1006.637, 1009.505, 990.419, 976.739, 949.181, 937.050, 925.810, 927.843, 919.324, 920.378, 923.239, 929.856, 874.345, 869.418, 844.642, 813.909, 782.993, 753.203, 724.175, 731.045, 750.315, 782.591, 804.119, 842.056, 869.630, 897.671, 916.529, 914.812, 880.695, 884.894, 828.311, 769.462, 720.269, 678.440, 650.350, 648.275]
      std1:  [377.969, 454.927, 509.648, 663.225, 4.606, 4.104]
      std2:  [262.658, 264.218, 266.238, 268.520, 271.314, 274.417, 277.690, 281.143, 283.609, 285.875, 290.075, 291.693, 292.987, 294.783, 297.264, 300.135, 304.173, 307.715, 311.216, 315.269, 319.666, 324.806, 330.629, 339.265, 342.287, 345.117, 347.390, 349.633, 349.462, 345.958, 341.043, 335.233, 330.310, 325.421, 321.263, 320.142, 319.573, 322.109, 324.541, 323.369, 313.437, 304.176, 295.106, 287.168, 275.492, 263.964, 255.904, 250.446, 248.273, 248.307, 249.869, 253.019, 256.607, 263.015, 268.048, 269.423, 268.091, 269.031, 271.468, 271.956, 280.045, 282.251, 280.115, 277.253, 279.424, 276.046, 262.960, 250.279, 237.595, 227.998, 219.871, 217.434, 217.565, 224.737, 242.565, 265.104, 287.018, 296.602, 304.602, 317.398, 332.026, 335.748, 332.952, 333.034, 330.232, 333.054, 335.839, 342.172, 343.393, 345.536, 345.421, 346.302, 323.282, 321.084, 311.701, 300.138, 288.921, 278.883, 270.019, 275.492, 286.234, 300.949, 310.969, 326.029, 336.290, 345.442, 350.181, 349.298, 335.476, 336.572, 314.432, 291.666, 273.035, 257.909, 248.256, 248.914]
  mode: train

val_dataset:
  type: RS_MD3B
  dataset_root: /home/aistudio/data/src/C2Seg_BW/train/
  val_path:  /home/aistudio/data/src/val.txt
  num_classes: 14
  transforms:
    - type: Normalize2
      mean1: [511.299, 664.097, 769.063, 1248.104, -15.968, -24.247]
      mean2: [999.433, 986.904, 974.447, 960.982, 947.575, 935.621, 924.243, 915.542, 907.726, 905.250, 909.091, 907.068, 902.661, 901.638, 902.661, 905.772, 911.386, 915.995, 920.723, 926.349, 931.746, 938.978, 946.329, 961.263, 959.789, 957.446, 954.460, 955.459, 954.627, 948.609, 945.025, 944.374, 949.510, 955.580, 960.178, 970.076, 975.216, 984.512, 990.831, 985.008, 950.914, 919.385, 889.296, 862.777, 825.307, 788.723, 762.220, 743.140, 734.277, 732.632, 733.673, 742.012, 753.558, 775.226, 794.269, 801.276, 799.636, 804.197, 816.373, 821.174, 854.051, 868.974, 869.269, 866.709, 876.802, 871.139, 835.212, 802.712, 767.638, 740.126, 713.945, 702.162, 696.747, 712.822, 763.065, 827.185, 896.042, 921.248, 940.171, 972.243, 1006.637, 1009.505, 990.419, 976.739, 949.181, 937.050, 925.810, 927.843, 919.324, 920.378, 923.239, 929.856, 874.345, 869.418, 844.642, 813.909, 782.993, 753.203, 724.175, 731.045, 750.315, 782.591, 804.119, 842.056, 869.630, 897.671, 916.529, 914.812, 880.695, 884.894, 828.311, 769.462, 720.269, 678.440, 650.350, 648.275]
      std1:  [377.969, 454.927, 509.648, 663.225, 4.606, 4.104]
      std2:  [262.658, 264.218, 266.238, 268.520, 271.314, 274.417, 277.690, 281.143, 283.609, 285.875, 290.075, 291.693, 292.987, 294.783, 297.264, 300.135, 304.173, 307.715, 311.216, 315.269, 319.666, 324.806, 330.629, 339.265, 342.287, 345.117, 347.390, 349.633, 349.462, 345.958, 341.043, 335.233, 330.310, 325.421, 321.263, 320.142, 319.573, 322.109, 324.541, 323.369, 313.437, 304.176, 295.106, 287.168, 275.492, 263.964, 255.904, 250.446, 248.273, 248.307, 249.869, 253.019, 256.607, 263.015, 268.048, 269.423, 268.091, 269.031, 271.468, 271.956, 280.045, 282.251, 280.115, 277.253, 279.424, 276.046, 262.960, 250.279, 237.595, 227.998, 219.871, 217.434, 217.565, 224.737, 242.565, 265.104, 287.018, 296.602, 304.602, 317.398, 332.026, 335.748, 332.952, 333.034, 330.232, 333.054, 335.839, 342.172, 343.393, 345.536, 345.421, 346.302, 323.282, 321.084, 311.701, 300.138, 288.921, 278.883, 270.019, 275.492, 286.234, 300.949, 310.969, 326.029, 336.290, 345.442, 350.181, 349.298, 335.476, 336.572, 314.432, 291.666, 273.035, 257.909, 248.256, 248.914]
  mode: val





# 682.8222280649038	904.1583766168727	855.6306981795873	2714.7835360237095	261.79564319322594	259.42994616536953	347.27061044533224	669.844619319619
# -15.958288192749023	-9.107815742492676	3.9245078563690186	4.6198015213012695

# 511.29928066136125	664.0968298113313	769.0629001884568	1248.1036750237815	377.96860512147674	454.92702423563065	509.64776482004817	663.2246271166581
# -15.96798038482666	-24.24662208557129	4.60582971572876	4.104198932647705

# 2.677734227705505	3.545719123987736	3.3554145026650484	10.646209945191018	1.0266495811499055	1.0173723379034099	1.3618455311581656	2.626841644390663
# -0.06258152980430454	-0.035716924480363434	0.015390226887721641	0.01811686684103573

# 0.26777342277055055	0.35457191239877356	0.33554145026650506	1.0646209945191019	0.10266495811499242	0.10173723379034107	0.13618455311581357	0.2626841644390632
# 0.6258153578814338	0.35716920740464153	0.15390227074716606	0.1811686347512638
